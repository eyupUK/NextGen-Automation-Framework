name: Performance Tests

on:
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of performance test'
        required: true
        default: 'load'
        type: choice
        options: [load, stress, spike]
      users:
        description: 'Number of concurrent users'
        required: false
        default: '10'
      duration:
        description: 'Test duration in seconds'
        required: false
        default: '60'
      simulation:
        description: 'Simulation to run'
        required: true
        default: 'weather'
        type: choice
        options: [weather, ecommerce, all]

  push:
    branches: [ main, master, complete-ci-implementation ]

permissions:
  contents: read
  issues: write
  pull-requests: write

concurrency:
  group: performance-tests-${{ github.ref }}
  cancel-in-progress: true

jobs:
  gatling-performance-tests:
    name: Gatling Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 45

    env:
      WEATHER_API_KEY: ${{ secrets.WEATHER_API_KEY }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Java 21 (build)
        uses: actions/setup-java@v4
        with:
          java-version: '21'
          distribution: 'temurin'
          cache: 'maven'

      - name: Cache Maven dependencies
        uses: actions/cache@v4
        with:
          path: ~/.m2/repository
          key: ${{ runner.os }}-maven-${{ hashFiles('**/pom.xml') }}
          restore-keys: |
            ${{ runner.os }}-maven-

      - name: Install dependencies (no tests)
        run: mvn -B -ntp clean install -DskipTests

      - name: Set up Java 17 (Gatling)
        uses: actions/setup-java@v4
        with:
          java-version: '17'
          distribution: 'temurin'

      - name: Recompile tests for JDK 17 (Gatling)
        run: |
          mvn -B -ntp clean test-compile \
            -DWEATHER_API_KEY="${WEATHER_API_KEY}" \
            -DskipTests \
            -Dmaven.compiler.testRelease=17

      - name: Set test parameters
        id: test-params
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "TEST_TYPE=${{ github.event.inputs.test_type }}" >> $GITHUB_ENV
            echo "USERS=${{ github.event.inputs.users }}" >> $GITHUB_ENV
            echo "DURATION=${{ github.event.inputs.duration }}" >> $GITHUB_ENV
            echo "SIMULATION=${{ github.event.inputs.simulation }}" >> $GITHUB_ENV
          else
            echo "TEST_TYPE=load" >> $GITHUB_ENV
            echo "USERS=10" >> $GITHUB_ENV
            echo "DURATION=60" >> $GITHUB_ENV
            echo "SIMULATION=weather" >> $GITHUB_ENV
          fi

      - name: Run Weather API Performance Test
        id: weather-perf
        if: env.SIMULATION == 'weather' || env.SIMULATION == 'all'
        run: |
          mvn -B -ntp gatling:test \
            -DWEATHER_API_KEY="${WEATHER_API_KEY}" \
            -Dgatling.simulationClass=com.example.performance.simulations.WeatherApiPerformanceSimulation \
            -Dperf.users=${{ env.USERS }} \
            -Dperf.duration=${{ env.DURATION }} \
            -Dperf.type=${{ env.TEST_TYPE }}
        continue-on-error: true

      - name: Run E-commerce API Performance Test
        id: ecommerce-perf
        if: env.SIMULATION == 'ecommerce' || env.SIMULATION == 'all'
        run: |
          mvn -B -ntp gatling:test \
            -Dgatling.simulationClass=com.example.performance.simulations.EcommerceApiPerformanceSimulation \
            -Dperf.users=${{ env.USERS }}
        continue-on-error: true

      - name: Debug - List target contents (Gatling)
        if: always()
        run: |
          echo "Listing target directory (if exists):" && ls -la target || true
          echo "\nGatling directories:" && ls -la target/gatling-results 2>/dev/null || true
          echo "\nAllure results:" && ls -la target/allure-results 2>/dev/null || true

      - name: Upload Gatling Reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: gatling-reports-${{ github.run_number }}
          path: target/gatling-results/
          retention-days: 30

      - name: Generate Performance Summary
        if: always()
        run: |
          echo "# 📊 Performance Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Test Configuration:**" >> $GITHUB_STEP_SUMMARY
          echo "- Test Type: ${{ env.TEST_TYPE }}" >> $GITHUB_STEP_SUMMARY
          echo "- Users: ${{ env.USERS }}" >> $GITHUB_STEP_SUMMARY
          echo "- Duration: ${{ env.DURATION }}s" >> $GITHUB_STEP_SUMMARY
          echo "- Simulation: ${{ env.SIMULATION }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Reports Generated:**" >> $GITHUB_STEP_SUMMARY
          echo "- Download artifacts to view detailed Gatling HTML reports" >> $GITHUB_STEP_SUMMARY
          echo "- Reports include: Response times, throughput, percentiles, charts" >> $GITHUB_STEP_SUMMARY

      - name: Finalize job status (Gatling)
        if: always()
        run: |
          # If any selected simulation failed, mark job failed (IDs may not exist if step skipped)
          W_CONCL='${{ steps.weather-perf.conclusion }}'
          E_CONCL='${{ steps.ecommerce-perf.conclusion }}'
          if [ "$W_CONCL" = "failure" ] || [ "$E_CONCL" = "failure" ]; then
            echo "Gatling run failed. Failing job." && exit 1
          fi

  junit-performance-tests:
    name: JUnit Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    env:
      WEATHER_API_KEY: ${{ secrets.WEATHER_API_KEY }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Java
        uses: actions/setup-java@v4
        with:
          java-version: '21'
          distribution: 'temurin'
          cache: 'maven'

      - name: Cache Maven dependencies
        uses: actions/cache@v4
        with:
          path: ~/.m2/repository
          key: ${{ runner.os }}-maven-${{ hashFiles('**/pom.xml') }}
          restore-keys: |
            ${{ runner.os }}-maven-

      - name: Install dependencies
        run: mvn -B -ntp clean install -DskipTests

      - name: Prepare performance results folder
        run: mkdir -p target/performance-results

      - name: Run JUnit Performance Tests
        id: junit-perf
        run: mvn -B -ntp test -DWEATHER_API_KEY="${WEATHER_API_KEY}" -Dtest=WeatherApiPerformanceTest -Dsla.p95.ms=2000 -Dsla.p99.ms=4000 -Dsla.successRate=95
        continue-on-error: true

      - name: Debug - List target contents (JUnit)
        if: always()
        run: |
          echo "Listing target directory (if exists):" && ls -la target || true
          echo "\nSurefire reports:" && ls -la target/surefire-reports 2>/dev/null || true
          echo "\nPerformance results:" && ls -la target/performance-results 2>/dev/null || true

      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: junit-performance-results-${{ github.run_number }}
          path: |
            target/surefire-reports/
            target/performance-results/
          retention-days: 30

      - name: Upload CSV Metrics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-metrics-csv-${{ github.run_number }}
          path: target/performance-results/*.csv
          retention-days: 90

      - name: Suite Summary - JUnit Performance
        if: always()
        run: |
          echo "# 🧪 Suite Summary - JUnit Performance" >> $GITHUB_STEP_SUMMARY
          echo "Result: ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
          echo "Artifacts: junit-performance-results-${{ github.run_number }}, performance-metrics-csv-${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY

      - name: Finalize job status (JUnit)
        if: always()
        run: |
          if [ "${{ steps.junit-perf.conclusion }}" = "failure" ]; then
            echo "JUnit performance run failed. Failing job." && exit 1
          fi

  performance-comparison:
    name: Performance Comparison
    runs-on: ubuntu-latest
    needs: [gatling-performance-tests]
    if: always()
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download Current Results
        uses: actions/download-artifact@v4
        with:
          name: gatling-reports-${{ github.run_number }}
          path: current-results/

      - name: Generate Comparison Report
        run: |
          echo "# 📈 Performance Comparison Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Run Information:**" >> $GITHUB_STEP_SUMMARY
          echo "- Run Number: #${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY
          echo "- Triggered by: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "- Branch: ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Next Steps:**" >> $GITHUB_STEP_SUMMARY
          echo "1. Download the artifacts to view detailed reports" >> $GITHUB_STEP_SUMMARY
          echo "2. Compare with baseline metrics" >> $GITHUB_STEP_SUMMARY
          echo "3. Investigate any performance regressions" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Artifacts Available:**" >> $GITHUB_STEP_SUMMARY
          echo "- 📊 Gatling HTML Reports (interactive charts)" >> $GITHUB_STEP_SUMMARY
          echo "- 📈 CSV Metrics (for trend analysis)" >> $GITHUB_STEP_SUMMARY
          echo "- 📝 Test Results (detailed logs)" >> $GITHUB_STEP_SUMMARY
